{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8adb3ab6",
   "metadata": {},
   "source": [
    "# Neural Network from scratch using only numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ef3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42 # set random seed to 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b26a1",
   "metadata": {},
   "source": [
    "Helper function to plot decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2f081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func, x_min, x_max, y_min, y_max, cmap, ax):\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.flatten(), yy.flatten()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e268a",
   "metadata": {},
   "source": [
    "## Neural Network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def len_check(y_pred,y_true):\n",
    "        prediction_dim = y_pred.shape\n",
    "        true_dim = y_true.shape\n",
    "        if (prediction_dim!=true_dim):\n",
    "            print(f'length of y_pred is {prediction_dim} while length of y_true is {true_dim}, there is a length mismatch. Raising error!')\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        row_wiseSummed = np.sum(y_true*np.log(y_pred))\n",
    "        loss = -np.mean(row_wiseSummed)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_loss(y_pred, y_true)->np.ndarray:\n",
    "        #1/N * summation (y_hat - y)^2\n",
    "        loss = np.mean((y_pred-y_true)**2)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true)->np.ndarray:\n",
    "        correct_preds = np.sum((y_pred==y_true),axis=1)\n",
    "        total_preds = len(y_pred)\n",
    "        accuracy = (correct_preds/total_preds)*100\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x)->np.ndarray:\n",
    "        x = x-np.max(x,axis=1,keepdims=True)\n",
    "        sum_ofExps = np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "        softmax_vals = np.exp(x)/sum_ofExps\n",
    "        return softmax_vals\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        x=np.clip(x,-500,500)\n",
    "        activations = 1/(1+np.exp(-x))\n",
    "        return activations\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tanh(x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        x = np.clip(x,-500,500)\n",
    "        activations = (np.exp(x)-np.exp(-x))/np.exp(x)+np.exp(-x)\n",
    "        return activations\n",
    "\n",
    "    @staticmethod\n",
    "    def ReLU(x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        activations = np.where(x>0,x,0)\n",
    "        return activations\n",
    "\n",
    "    def Leaky_ReLU(self, x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        activations = np.where(x>self.leaky_slope*x,x,self.leaky_slope*x)\n",
    "        return activations\n",
    "\n",
    "    def __init__(self, nodes_per_layer: list[int], mode: str = 'classification', optimizer: str = 'fullbatch', activation: str = 'sigmoid', batch_size: int = 64):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing the number of nodes in each layer (including the input layer) e.g. [2, 10, 10, 2] (2 features, 2 hidden layers with 10 neurons each, 2 classes)\"\n",
    "        '''\n",
    "\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have at least 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('The number of nodes in all layers must be positive.')\n",
    "\n",
    "        assert mode.lower() in ['classification', 'regression'], \"Only classification and regression modes are supported\"\n",
    "        assert optimizer.lower() in ['sgd', 'minibatch', 'fullbatch'], 'Unknown Optimizer'\n",
    "        assert activation.lower() in ['sigmoid', 'relu', 'leaky_relu','tanh'], 'Unknown activation function'\n",
    "\n",
    "        self.num_layers = np.size(nodes_per_layer)\n",
    "        self.mode = mode\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.input_shape = nodes_per_layer[0]\n",
    "        self.output_shape = nodes_per_layer[-1]\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_strat = activation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            self.leaky_slope = 0.05\n",
    "\n",
    "        # Initialize all weights based on a standard normal distribution and all biases to 0.\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        self.__init_weights(nodes_per_layer)\n",
    "\n",
    "\n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on a standard normal distribution and all biases to 0.'''\n",
    "        np.random.seed(SEED)\n",
    "        for i, _ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip the input layer; it does not have weights/bias\n",
    "                continue\n",
    "\n",
    "            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))\n",
    "            self.weights_.append(weight_matrix)\n",
    "            bias_vector = np.zeros(shape=(nodes_per_layer[i],))\n",
    "            self.biases_.append(bias_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, Xs, Ys, X_val, Y_val, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\".\n",
    "        Returns list containing loss for each epoch.'''\n",
    "        self.history = []\n",
    "        self.val_history = []\n",
    "        self._run_optimizer(Xs,Ys, X_val, Y_val, epochs, lr)\n",
    "        return self.history, self.val_history\n",
    "\n",
    "\n",
    "    def _run_optimizer(self, Xs, Ys, X_val, Y_val, epochs, lr):\n",
    "        ''' Executes the gradient descent algorithm '''\n",
    "        # Add functionality for converting integer labels to one-hot labels (optional)\n",
    "\n",
    "        if self.optimizer == 'fullbatch':\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                activations = self.forward_pass(Xs) # Forward Pass over whole dataset\n",
    "                deltas = self.backward_pass(Ys, activations) # Backward pass takes layer activations and true labels as input\n",
    "\n",
    "                layer_inputs = [Xs] + activations[:-1]\n",
    "                self.weight_update(deltas, layer_inputs, lr) # Weight update\n",
    "\n",
    "                preds = self.predict(Xs) # Make prediction on input sample\n",
    "\n",
    "                current_loss = self.cross_entropy_loss(preds, Ys) if self.mode == 'classification' else self.mean_squared_loss(preds, Ys)\n",
    "                self.history.append(current_loss)\n",
    "\n",
    "                # Compute Validation Loss at the end of each epoch\n",
    "                preds = self.predict(X_val)\n",
    "                current_loss = self.cross_entropy_loss(preds, Y_val) if self.mode == 'classification' else self.mean_squared_loss(preds, Y_val)\n",
    "                self.val_history.append(current_loss)\n",
    "\n",
    "        elif self.optimizer == 'sgd':\n",
    "             # Code Here\n",
    "             pass\n",
    "\n",
    "\n",
    "        elif self.optimizer == 'minibatch':\n",
    "            # Code Here\n",
    "            pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "\n",
    "        # Code here\n",
    "\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropagation algorithm.\n",
    "        \"targets\" is the ground truth/labels.\n",
    "        \"layer_activations\" are the return value of the forward pass step.\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course).'''\n",
    "\n",
    "        # Code here\n",
    "\n",
    "        return deltas\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "\n",
    "        # Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        return self.forward_pass(Xs)[-1]\n",
    "\n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = ...\n",
    "        if self.mode == 'regression': # return MSE\n",
    "            return ...\n",
    "        elif self.mode == 'classification': # return CE_Loss and accuracy in that order\n",
    "            return ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
