{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8adb3ab6",
   "metadata": {},
   "source": [
    "# Neural Network from scratch using only numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ef3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42 # set random seed to 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b26a1",
   "metadata": {},
   "source": [
    "Helper function to plot decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2f081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(pred_func, x_min, x_max, y_min, y_max, cmap, ax):\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.flatten(), yy.flatten()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e268a",
   "metadata": {},
   "source": [
    "## Neural Network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def len_check(y_pred,y_true):\n",
    "        prediction_dim = y_pred.shape\n",
    "        true_dim = y_true.shape\n",
    "        if (prediction_dim!=true_dim):\n",
    "            print(f'length of y_pred is {prediction_dim} while length of y_true is {true_dim}, there is a length mismatch. Raising error!')\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true)->float:\n",
    "        #Must be passed the softmax activation\n",
    "        row_wiseSummed = np.sum(y_true*np.log(y_pred + 1e-12),axis=1)\n",
    "        loss = -np.mean(row_wiseSummed)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_CE(y_pred,y_true)->np.ndarray:\n",
    "        grad = (y_pred-y_true)/y_true.shape[0]\n",
    "        return grad\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_loss(y_pred, y_true)->float:\n",
    "        #1/N * summation (y_hat - y)^2\n",
    "        loss = np.mean((y_pred-y_true)**2)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradMSE(y_pred,y_true)->np.ndarray:\n",
    "        grad = 2*(y_pred-y_true)/y_true.shape[0]\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true)->np.ndarray:\n",
    "        correct_preds = np.sum((y_pred.flatten()==y_true.flatten()))\n",
    "        total_preds = len(y_pred)\n",
    "        accuracy = (correct_preds/total_preds)*100\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x)->np.ndarray:\n",
    "        x = x-np.max(x,axis=1,keepdims=True)\n",
    "        sum_ofExps = np.sum(np.exp(x),axis=1,keepdims=True)\n",
    "        softmax_vals = np.exp(x)/sum_ofExps\n",
    "        return softmax_vals\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        x=np.clip(x,-500,500)\n",
    "        activations = 1/(1+np.exp(-x))\n",
    "        return activations\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(sigmoid)->np.ndarray:\n",
    "        grad = sigmoid*(1-sigmoid)\n",
    "        return grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def Tanh(x)->np.ndarray:\n",
    "        #clipping results for numerical stability\n",
    "        x = np.clip(x,-500,500)\n",
    "        activations = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return activations\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_Tanh(tanh)->np.ndarray:\n",
    "        grad = 1-(tanh**2)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def ReLU(x)->np.ndarray:\n",
    "        activations = np.where(x>0,x,0)\n",
    "        return activations\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_ReLU(x)->np.ndarray:\n",
    "        grad = np.where(x>0,1,0)\n",
    "        return grad\n",
    "\n",
    "    def Leaky_ReLU(self, x)->np.ndarray:\n",
    "        activations = np.where(x>0,x,self.leaky_slope*x)\n",
    "        return activations\n",
    "    \n",
    "    def grad_LeakyRELU(self,x)->np.ndarray:\n",
    "        grad = np.where(x>0,1,self.leaky_slope)\n",
    "        return grad\n",
    "    \n",
    "    def __init__(self, nodes_per_layer: list[int], mode: str = 'classification', optimizer: str = 'fullbatch', activation: str = 'sigmoid', batch_size: int = 64):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing the number of nodes in each layer (including the input layer) e.g. [2, 10, 10, 2] (2 features, 2 hidden layers with 10 neurons each, 2 classes)\"\n",
    "        '''\n",
    "\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have at least 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('The number of nodes in all layers must be positive.')\n",
    "\n",
    "        assert mode.lower() in ['classification', 'regression'], \"Only classification and regression modes are supported\"\n",
    "        assert optimizer.lower() in ['sgd', 'minibatch', 'fullbatch'], 'Unknown Optimizer'\n",
    "        assert activation.lower() in ['sigmoid', 'relu', 'leaky_relu','tanh'], 'Unknown activation function'\n",
    "\n",
    "        self.num_layers = np.size(nodes_per_layer)\n",
    "        self.mode = mode\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.input_shape = nodes_per_layer[0]\n",
    "        self.output_shape = nodes_per_layer[-1]\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_strat = activation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            self.leaky_slope = 0.05\n",
    "\n",
    "        # Initialize all weights based on a standard normal distribution and all biases to 0.\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        self.__init_weights(nodes_per_layer)\n",
    "\n",
    "\n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on a standard normal distribution and all biases to 0.'''\n",
    "        np.random.seed(SEED)\n",
    "        for i, _ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip the input layer; it does not have weights/bias\n",
    "                continue\n",
    "\n",
    "            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))\n",
    "            self.weights_.append(weight_matrix)\n",
    "            bias_vector = np.zeros(shape=(nodes_per_layer[i],))\n",
    "            self.biases_.append(bias_vector)\n",
    "\n",
    "\n",
    "    def activaton(self,z):\n",
    "        if self.activation_strat.lower() == 'sigmoid':\n",
    "            return NeuralNetwork.sigmoid(z)\n",
    "        elif self.activation_strat.lower() == 'tanh':\n",
    "            return NeuralNetwork.Tanh(z)\n",
    "        elif self.activation_strat.lower() == 'relu':\n",
    "            return NeuralNetwork.ReLU(z)\n",
    "        elif self.activation_strat.lower() == 'leaky_relu':\n",
    "            return self.Leaky_ReLU(z)\n",
    "        \n",
    "    def grad_activation(self,activation=0,pre_activation=0):\n",
    "        if self.activation_strat.lower() == 'sigmoid':\n",
    "            return NeuralNetwork.grad_sigmoid(activation)\n",
    "        elif self.activation_strat.lower() == 'tanh':\n",
    "            return NeuralNetwork.grad_Tanh(activation)\n",
    "        elif self.activation_strat.lower() == 'relu':\n",
    "            return NeuralNetwork.grad_ReLU(pre_activation)\n",
    "        elif self.activation_strat.lower() == 'leaky_relu':\n",
    "            return self.grad_LeakyRELU(pre_activation)\n",
    "        \n",
    "    def Loss_func(self,y_pred,y_true):\n",
    "        if self.mode.lower() == 'classification':\n",
    "            y_pred = self.softmax(y_pred)\n",
    "            return NeuralNetwork.cross_entropy_loss(y_pred,y_true)\n",
    "        else:\n",
    "            return NeuralNetwork.mean_squared_loss(y_pred,y_true)\n",
    "        \n",
    "    def Loss_funcGrad(self,y_pred,y_true):\n",
    "        if self.mode.lower() == 'classification':\n",
    "            y_pred=self.softmax(y_pred)\n",
    "            return NeuralNetwork.grad_CE(y_pred,y_true)\n",
    "        else:\n",
    "            return NeuralNetwork.gradMSE(y_pred,y_true)\n",
    "        \n",
    "    def makeOneHot(self,Y):\n",
    "        total_classes = np.unique(Y).shape[0]\n",
    "        size = len(Y)\n",
    "        onehot = np.zeros((size,total_classes))\n",
    "        for idx in range(size):\n",
    "            onehot[idx][Y[idx]] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def create_batches(self,x_train,y_train):\n",
    "      indices = np.arange(x_train.shape[0])\n",
    "      np.random.shuffle(indices)\n",
    "      batches_needed = x_train.shape[0]//self.batch_size\n",
    "      batches = []\n",
    "      for i in range(0,x_train.shape[0],self.batch_size):\n",
    "        end_point = i+self.batch_size\n",
    "        batch_indices = indices[i:end_point]\n",
    "        x_batch = x_train[batch_indices]\n",
    "        x_batch = x_batch.reshape(len(x_batch), -1)\n",
    "        y_batch = y_train[batch_indices]\n",
    "        if y_batch.ndim == 1 or y_batch.shape[1] == 1:\n",
    "            y_batch = y_batch.reshape(-1, 1)\n",
    "        batches.append((x_batch,y_batch))\n",
    "        if(len(batches)==batches_needed):\n",
    "          break\n",
    "      return batches\n",
    "\n",
    "    def fit(self, Xs, Ys, X_val, Y_val, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\".\n",
    "        Returns list containing loss for each epoch.'''\n",
    "        self.history = []\n",
    "        self.val_history = []\n",
    "        self._run_optimizer(Xs,Ys, X_val, Y_val, epochs, lr)\n",
    "        return self.history, self.val_history\n",
    "    \n",
    "    def _run_optimizer(self, Xs, Ys, X_val, Y_val, epochs, lr):\n",
    "        ''' Executes the gradient descent algorithm '''\n",
    "        # Add functionality for converting integer labels to one-hot labels (optional)\n",
    "        if self.mode == 'classification' and self.nodes_per_layer[-1]>1:\n",
    "            Ys = self.makeOneHot(Ys)\n",
    "\n",
    "        if self.optimizer == 'fullbatch':\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                activations = self.forward_pass(Xs) # Forward Pass over whole dataset\n",
    "                deltas = self.backward_pass(Ys, activations,Xs) # Backward pass takes layer activations and true labels as input\n",
    "                self.parameter_update(deltas,lr) # Weight update\n",
    "                preds = self.forward_pass(Xs)[-1][1] # Make prediction on input sample\n",
    "                current_loss = self.Loss_func(preds,Ys)\n",
    "                print(f'epoch {epoch} training loss = {current_loss}')\n",
    "                self.history.append(current_loss)\n",
    "                # Compute Validation Loss at the end of each epoch\n",
    "                val_loss,acc = self.evaluate(X_val,Y_val)\n",
    "                print(f'epoch {epoch} val loss={val_loss}, val acc = {acc}')\n",
    "                self.val_history.append(val_loss)\n",
    "\n",
    "        elif self.optimizer == 'sgd':\n",
    "            # Code Here\n",
    "            total_idx = Xs.shape[0]\n",
    "            idxs = np.arange(total_idx)\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                epoch_loss = 0\n",
    "                np.random.shuffle(idxs)\n",
    "                for i in idxs:\n",
    "                    X = Xs[i:i+1]\n",
    "                    Y = Ys[i:i+1]\n",
    "                    activations = self.forward_pass(X)\n",
    "                    deltas = self.backward_pass(Y, activations,X)\n",
    "                    self.parameter_update(deltas,lr)\n",
    "                    preds = self.forward_pass(X)[-1][1]\n",
    "                    current_loss = self.Loss_func(preds,Y)\n",
    "                    epoch_loss += current_loss\n",
    "                print(f'epoch {epoch} training loss = {epoch_loss}')\n",
    "                self.history.append(epoch_loss)\n",
    "\n",
    "                val_loss,acc = self.evaluate(X_val,Y_val)\n",
    "                print(f'epoch {epoch} val loss={val_loss}, val acc = {acc}')\n",
    "                self.val_history.append(val_loss)\n",
    "\n",
    "\n",
    "        elif self.optimizer == 'minibatch':\n",
    "            # Code Here\n",
    "            batches = self.create_batches(Xs,Ys)\n",
    "            batches = np.array(batches)\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                np.random.shuffle(batches)\n",
    "                epoch_loss=0\n",
    "                for batch in batches:\n",
    "                    X,Y = batch\n",
    "                    activations = self.forward_pass(X)\n",
    "                    deltas = self.backward_pass(Y, activations,X)\n",
    "                    self.parameter_update(deltas,lr)\n",
    "                    preds = self.forward_pass(X)[-1][1]\n",
    "                    current_loss = self.Loss_func(preds,Y)\n",
    "                    epoch_loss += current_loss\n",
    "                print(f'epoch {epoch} training loss = {epoch_loss}')\n",
    "\n",
    "                self.history.append(epoch_loss)\n",
    "                val_loss,acc = self.evaluate(X_val,Y_val)\n",
    "                print(f'epoch {epoch} val loss={val_loss}, val acc = {acc}')\n",
    "                self.val_history.append(val_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "\n",
    "        # Code here\n",
    "        #input is in row major form so every row represents a sample and every column a feature. nxd\n",
    "        #weight matrix is (nodes in prev layer, nodes in target layer) d x p.\n",
    "        # z1 = p x n\n",
    "        # z_n = (pxne).T x pxn = nexp x pxn = weights.T@n\n",
    "        #.T when adding to activations so that everything is stored in row major form.\n",
    "        z1 = self.weights_[0].T @ input_data.T + self.biases_[0]\n",
    "        a1 = self.activaton(z1)\n",
    "        activations = []\n",
    "        activations.append((z1.T,a1.T))\n",
    "        a=a1\n",
    "        for weight,bias in zip(self.weights_[1:],self.biases_[1:]):\n",
    "            z = weight.T@a + bias\n",
    "            a = self.activaton(z)\n",
    "            activations.append((z.T,a.T))\n",
    "\n",
    "        if self.mode.lower() == 'regression':\n",
    "            # In the last layer of regression we dont want an activation function applying to the final z value rather we want an indentity activation. this does that\n",
    "            last_layerTuple = activations[-1]\n",
    "            z_L = last_layerTuple[0]\n",
    "            activations[-1] = (z_L,z_L)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward_pass(self, targets, layer_activations, input):\n",
    "        '''Executes the backpropagation algorithm.\n",
    "        \"targets\" is the ground truth/labels.\n",
    "        \"layer_activations\" are the return value of the forward pass step.\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course).'''\n",
    "        # Code here\n",
    "        \"\"\"\"Has three components:error signal, d with respect to weight, d with respect to bias\n",
    "        starting off we have error signal which is (nxd(i)). and da_n/dz_n which nxd(i). dz/da_n-1 which is (d(i-1),d(i))\n",
    "        error signal = error_signal*da_n/dz_n@dz_n/da_n-1.\n",
    "        dw = error_signal*da_n/dz_n*dz_n/dw_n\n",
    "        db = sum the error per every bias so sum alog the rows\n",
    "        \"\"\"\n",
    "\n",
    "        deltas = []\n",
    "        y_pred = (layer_activations[-1])[1]\n",
    "        errorSignal = self.Loss_funcGrad(y_pred,targets)\n",
    "        for i in reversed(range(len(self.weights_))):\n",
    "            z,a = layer_activations[i]\n",
    "            weight = self.weights_[i]\n",
    "            activation_grad= self.grad_activation(a,z)\n",
    "            errorSignal = errorSignal*activation_grad\n",
    "            #dw\n",
    "            if (i==0):\n",
    "                dw = input.T @ errorSignal\n",
    "            else:\n",
    "                dw = layer_activations[i-1][1].T @ errorSignal\n",
    "            #db\n",
    "            db = np.sum(errorSignal, axis=0, keepdims=True).T\n",
    "            #error signal part\n",
    "            errorSignal = errorSignal@weight.T\n",
    "            deltas.append((dw,db))\n",
    "\n",
    "        deltas.reverse()\n",
    "        return deltas\n",
    "\n",
    "    def parameter_update(self, deltas, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "        # Code here\n",
    "        for i in range(len(deltas)):\n",
    "            dw_i,db_i = deltas[i]\n",
    "            self.weights_[i] = self.weights_[i] - lr*dw_i\n",
    "            self.biases_[i] = self.biases_[i] - lr*db_i\n",
    "\n",
    "\n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        predictions = self.forward_pass(Xs)[-1][1]\n",
    "        if self.mode.lower() == 'classification':\n",
    "            predictions = np.argmax(predictions,axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        if self.mode == 'regression': # return MSE\n",
    "            pred = self.forward_pass(Xs)[-1][1]\n",
    "            return self.Loss_func(pred,Ys)\n",
    "        elif self.mode == 'classification': # return CE_Loss and accuracy in that order\n",
    "            y_true = self.makeOneHot(Ys)\n",
    "            y_pred = self.forward_pass(Xs)[-1][1]\n",
    "            loss = self.Loss_func(y_pred,y_true)\n",
    "            predictions = np.argmax(y_pred,axis=1)\n",
    "            acc = self.accuracy(predictions,Ys)\n",
    "            return loss,acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
